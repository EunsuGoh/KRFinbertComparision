from transformers import AutoTokenizer, AutoModel
import torch
import faiss
import numpy as np
import pandas as pd
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# âœ… ëª¨ë¸ ì„¤ì •
models = {
    "FinBERT-KR": "snunlp/KR-FinBert-SC",
    "Kakao DEBERTa": "kakaobank/kf-deberta-base"
}

# âœ… í† í¬ë‚˜ì´ì € & ëª¨ë¸ ë¡œë“œ
tokenizers = {name: AutoTokenizer.from_pretrained(model) for name, model in models.items()}
models = {name: AutoModel.from_pretrained(model) for name, model in models.items()}

# âœ… FAQ ë°ì´í„° ë¡œë“œ
faq_file = "./faq.xlsx"  # FAQ íŒŒì¼ ê²½ë¡œ (í•„ìš”ì— ë§ê²Œ ë³€ê²½)
df = pd.read_excel(faq_file)
questions = df["Question"].tolist()

# # âœ… ì„ë² ë”© í•¨ìˆ˜
# def get_embedding(model, tokenizer, text):
#     inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
#     with torch.no_grad():
#         outputs = model(**inputs)
#     return outputs.last_hidden_state[:, 0, :].squeeze().numpy()

# âœ… í‰ê·  í’€ë§ ë°©ì‹ì„ ì ìš©í•œ ì„ë² ë”© í•¨ìˆ˜
def get_embedding(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # í‰ê·  í’€ë§ ë°©ì‹ ì ìš©
    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

# âœ… ëª¨ë“  ì§ˆë¬¸ ë²¡í„°í™”
embeddings = {name: [] for name in models.keys()}

for model_name, model in models.items():
    tokenizer = tokenizers[model_name]
    print(f"ğŸ”¹ {model_name} ì„ë² ë”© ìƒì„± ì¤‘...")
    
    for question in questions:
        emb = get_embedding(model, tokenizer, question)
        embeddings[model_name].append(emb)

    embeddings[model_name] = np.array(embeddings[model_name])

# âœ… FAISS ë²¡í„° DB ìƒì„± ë° ì €ì¥
index_faiss = {name: faiss.IndexFlatL2(768) for name in models.keys()}  # 768ì°¨ì› ë²¡í„°

for model_name in models.keys():
    index_faiss[model_name].add(embeddings[model_name])

print("âœ… FAISS ë²¡í„° DB ì €ì¥ ì™„ë£Œ!")

# âœ… ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ í•¨ìˆ˜
def search_similar(model_name, query, top_k=5):
    tokenizer = tokenizers[model_name]
    model = models[model_name]
    
    query_vec = get_embedding(model, tokenizer, query).reshape(1, -1)
    D, I = index_faiss[model_name].search(query_vec, top_k)  # ê±°ë¦¬(D), ì¸ë±ìŠ¤(I) ë°˜í™˜

    print(f"\nğŸ” {model_name} ê²€ìƒ‰ ê²°ê³¼:")
    for i, idx in enumerate(I[0]):
        print(f"{i+1}. {questions[idx]} (ìœ ì‚¬ë„ ì ìˆ˜: {D[0][i]:.4f})")

# # âœ… í…ŒìŠ¤íŠ¸: ì„ì˜ì˜ ì§ˆë¬¸ ê²€ìƒ‰
# test_query = "í‡´ì§ì—°ê¸ˆ ê³„ì¢Œ ì…ê¸ˆ ì‹œê°„ì´ ê¶ê¸ˆí•´ìš”."
# print(f"ğŸ” í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_query}")
# search_similar("FinBERT-KR", test_query)
# search_similar("Kakao DEBERTa", test_query)


test_queries = [
    "ì¸í„°ë„· ë±…í‚¹ ë¹„ë°€ë²ˆí˜¸ë¥¼ ë³€ê²½í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
    "í‡´ì§ì—°ê¸ˆ ê³„ì¢Œì—ì„œ ì¶œê¸ˆí•  ìˆ˜ ìˆë‚˜ìš”?",
    "ìë™ì´ì²´ ì„¤ì •ì„ ë³€ê²½í•˜ê³  ì‹¶ì€ë° ì–´ë””ì„œ í•˜ë‚˜ìš”?",
    "ì£¼ì‹ ê³„ì¢Œë¥¼ ê°œì„¤í•˜ë ¤ë©´ ì–´ë–¤ ì„œë¥˜ê°€ í•„ìš”í•œê°€ìš”?",
    "ì‹ ìš©ì¹´ë“œ í•œë„ë¥¼ ìƒí–¥ ì¡°ì •í•˜ëŠ” ë°©ë²•ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.",
    "ì •ê¸°ì˜ˆê¸ˆ ê¸ˆë¦¬ë¥¼ í™•ì¸í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
    "ëª¨ë°”ì¼ ì•±ì—ì„œ ëŒ€ì¶œ ì‹ ì²­ì´ ê°€ëŠ¥í•œê°€ìš”?",
    "IRP ê³„ì¢Œì—ì„œ ë‹¤ë¥¸ ê³„ì¢Œë¡œ ì´ì²´í•  ìˆ˜ ìˆë‚˜ìš”?",
    "í•´ì™¸ ê²°ì œ ì‹œ ì ìš©ë˜ëŠ” í™˜ìœ¨ì€ ì–´ë–»ê²Œ ê²°ì •ë˜ë‚˜ìš”?",
    "ìŠ¤ë§ˆíŠ¸í°ì—ì„œ ê³µì¸ì¸ì¦ì„œë¥¼ ë“±ë¡í•˜ëŠ” ë°©ë²•ì„ ì•Œê³  ì‹¶ì–´ìš”."
]
# âœ… ì—¬ëŸ¬ ê°œì˜ ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
def batch_search(queries, model_name, top_k=3):
    print(f"\nğŸš€ [{model_name}] ëª¨ë¸ ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ\n" + "="*50)
    
    for i, query in enumerate(queries):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {i+1}: {query}")
        search_similar(model_name, query, top_k=top_k)
        print("-"*50)

# âœ… ë‘ ëª¨ë¸ì— ëŒ€í•´ ë¹„êµ ì‹¤í–‰
batch_search(test_queries, "FinBERT-KR")
batch_search(test_queries, "Kakao DEBERTa")

# # ëª¨ë¸ ì„ë² ë”© ì°¨ì› í™•ì¸ -> clear 
# for model_name, model in models.items():
#     tokenizer = tokenizers[model_name]
#     dummy_input = tokenizer("í…ŒìŠ¤íŠ¸ ì…ë ¥", return_tensors="pt")
#     with torch.no_grad():
#         output = model(**dummy_input)
#     print(f"{model_name} ì„ë² ë”© ì°¨ì›: {output.last_hidden_state.shape[-1]}")

